### 定義
 * 梯度計算依賴於鏈式法則（Chain Rule）
 * 每一層的梯度是前一層(靠近output layer)梯度的乘積
---
### 權重更新
1. 計算完損失函數
2. 計算損失函數對於某個權重的偏導數
	* 偏導數反映了該權重對損失函數的影響程度
	* 偏導數是正的: 
		* 增加這個權重會增加損失 -> 需要減少這個權重的值
	- 偏導數是負的:
		- 增加這個權重會減少損失 -> 需要增加這個權重的值
3. 更新權重
	* w = w - 偏導數 * learning_rate
---
### 常見問題
#### 梯度消失
* 問題:
	* 如果導數小於1，導數連乘後的梯度快速接近0，導致梯度消失
	* 導致小梯度對模型權重的影響非常小
* 解決方式:
	* 使用ReLU之類的激勵函數，因為導數是常數，可以保持梯度大小
#### 梯度爆炸
* 問題:
	* 如果導數都大於1，導數連乘後的結果會迅速增大，導致梯度爆炸
	* 導致模型不穩定甚至無法收斂
* 解決方式:
	* 梯度剪裁
		* 設置最大梯度，避免無限增加
	* 使用LSTM, GRU
		* 有控制訊息的門，使模型更好捕捉長期資訊
	* 正規化
		* 極端值會導致Sigmod和Tanh導數變得很小